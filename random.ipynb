{"cells":[{"cell_type":"markdown","id":"a0ca23dc","metadata":{"id":"a0ca23dc"},"source":["## Random"]},{"cell_type":"markdown","id":"a5c5e162","metadata":{"id":"a5c5e162"},"source":["### Configuración Inicial"]},{"cell_type":"code","source":["!pip uninstall -y numpy\n","!pip install numpy==1.26"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"eoht7K8UUvuL","outputId":"7efd1ed3-7038-4ed7-a989-d1c366876c3f","executionInfo":{"status":"ok","timestamp":1764995576535,"user_tz":180,"elapsed":16887,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}}},"id":"eoht7K8UUvuL","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.0\n","Uninstalling numpy-1.26.0:\n","  Successfully uninstalled numpy-1.26.0\n","Collecting numpy==1.26\n","  Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n","Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n","Installing collected packages: numpy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.0 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.0 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.0 which is incompatible.\n","music21 9.9.1 requires numpy>=1.26.4, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"daa7169fba2b4ccea58b93b990809b39"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install scikit-surprise --no-build-isolation --no-deps\n","!pip install memory_profiler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8xY-LUmUzjr","outputId":"7adedc71-c8ee-46b3-d2d1-5d6f7eeb769e","executionInfo":{"status":"ok","timestamp":1764995641189,"user_tz":180,"elapsed":64628,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}}},"id":"x8xY-LUmUzjr","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-surprise\n","  Using cached scikit_surprise-1.1.4.tar.gz (154 kB)\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: scikit-surprise\n","  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2708540 sha256=717d0581bacbe93e57ca9481a1cac11257a938017e6fc64eedb27c2d6ce27aec\n","  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n","Successfully built scikit-surprise\n","Installing collected packages: scikit-surprise\n","Successfully installed scikit-surprise-1.1.4\n","Collecting memory_profiler\n","  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from memory_profiler) (5.9.5)\n","Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n","Installing collected packages: memory_profiler\n","Successfully installed memory_profiler-0.61.0\n"]}]},{"cell_type":"markdown","id":"c3f95148","metadata":{"id":"c3f95148"},"source":["### Instalación de Librerías"]},{"cell_type":"code","execution_count":3,"id":"80bca851","metadata":{"id":"80bca851","executionInfo":{"status":"ok","timestamp":1764995643685,"user_tz":180,"elapsed":2489,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}}},"outputs":[],"source":["import time\n","import json\n","import math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import defaultdict, Counter\n","from memory_profiler import memory_usage\n","import itertools\n","import scipy.sparse as sparse\n","import random\n","import gdown\n","from surprise import SVDpp, Dataset, Reader, accuracy\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from sklearn.metrics.pairwise import cosine_similarity\n","from surprise import SVD, Dataset, Reader\n","\n"]},{"cell_type":"markdown","id":"e0ae5cb5","metadata":{"id":"e0ae5cb5"},"source":["### Importación de los Datos"]},{"cell_type":"code","source":["gdown.download(id='1eGDDR1wlvR99eoCZG2owChy2dhkPp4yx', output='training_ratings.csv', quiet=False)\n","gdown.download(id='1oHo9HLB6SzeqZs76FCkfQ1irSQepqp16', output='validation_ratings.csv', quiet=False)"],"metadata":{"id":"OaUSf9OapGSh","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1764995652404,"user_tz":180,"elapsed":8714,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"78ec7e62-0e37-47f0-b2af-62a4e371be70"},"id":"OaUSf9OapGSh","execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1eGDDR1wlvR99eoCZG2owChy2dhkPp4yx\n","From (redirected): https://drive.google.com/uc?id=1eGDDR1wlvR99eoCZG2owChy2dhkPp4yx&confirm=t&uuid=43e68523-05df-4363-83e5-460731b989cc\n","To: /content/training_ratings.csv\n","100%|██████████| 205M/205M [00:01<00:00, 187MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1oHo9HLB6SzeqZs76FCkfQ1irSQepqp16\n","To: /content/validation_ratings.csv\n","100%|██████████| 64.4M/64.4M [00:00<00:00, 69.9MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'validation_ratings.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"id":"8dd256b7","metadata":{"id":"8dd256b7","executionInfo":{"status":"ok","timestamp":1764995656207,"user_tz":180,"elapsed":3799,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}}},"outputs":[],"source":["df_train = pd.read_csv('training_ratings.csv')\n","df_val = pd.read_csv('validation_ratings.csv')"]},{"cell_type":"code","source":["# dataset mechanics\n","gdown.download(id='1cVGSLNVqxrAoKzeqxt_FfQ4Ggs9VvCDO', output='mechanics.csv', quiet=False)\n","df_mechanics = pd.read_csv('mechanics.csv')"],"metadata":{"id":"VD18bHhEpd7m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764995658116,"user_tz":180,"elapsed":1904,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"0a5fd805-db76-4415-a8e8-51dc24f4d704"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1cVGSLNVqxrAoKzeqxt_FfQ4Ggs9VvCDO\n","To: /content/mechanics.csv\n","100%|██████████| 7.05M/7.05M [00:00<00:00, 79.8MB/s]\n"]}],"id":"VD18bHhEpd7m"},{"cell_type":"markdown","id":"62baea42","metadata":{"id":"62baea42"},"source":["### Preprocesamiento de Datos"]},{"cell_type":"code","source":["df_mechanics = pd.read_csv('mechanics.csv')\n","# Usamos BGGId como índice para que la búsqueda sea rápida\n","df_mechanics.set_index('BGGId', inplace=True)\n","print(\"Datos de mecánicas cargados y listos.\")\n","\n","# --- Calcular la popularidad de los ítems ---\n","# Usamos el dataframe de entrenamiento COMPLETO (df_train) para obtener una\n","# medida de popularidad global y precisa.\n","item_popularity = df_train['item'].value_counts().to_dict()\n","total_interactions = len(df_train)\n","\n","# Convertimos las cuentas en probabilidades para el cálculo de novedad\n","item_popularity_prob = {item_id: count / total_interactions for item_id, count in item_popularity.items()}\n","print(f\"Popularidad calculada para {len(item_popularity)} ítems.\")"],"metadata":{"id":"j_TroMS362w5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764995658302,"user_tz":180,"elapsed":183,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"787606b7-952f-42ae-b04f-fd612141b7dd"},"id":"j_TroMS362w5","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Datos de mecánicas cargados y listos.\n","Popularidad calculada para 16748 ítems.\n"]}]},{"cell_type":"code","source":["def novelty_at_k(group, k, popularity_prob):\n","    \"\"\"Calcula la Novedad@K para un solo usuario/grupo.\"\"\"\n","    group = group.sort_values('score', ascending=False)\n","    topk_items = group.head(k)['itemID']\n","\n","    novelty_scores = []\n","    for item_id in topk_items:\n","        # Si un ítem no está en el diccionario de popularidad, se le asigna una probabilidad muy baja\n","        prob = popularity_prob.get(item_id, 1e-6)\n","        novelty_scores.append(-np.log2(prob))\n","\n","    return np.mean(novelty_scores) if novelty_scores else 0.0\n","\n","def diversity_at_k(group, k, mechanics_df):\n","    \"\"\"Calcula la Diversidad@K (Intra-List Diversity) para un solo usuario/grupo.\"\"\"\n","    group = group.sort_values('score', ascending=False)\n","    topk_items = group.head(k)['itemID'].tolist()\n","\n","    # Nos aseguramos de que los ítems recomendados tengan datos de mecánicas\n","    topk_items = [item for item in topk_items if item in mechanics_df.index]\n","\n","    if len(topk_items) < 2:\n","        return 0.0\n","\n","    item_vectors = mechanics_df.loc[topk_items].values\n","\n","    # Calculamos la disimilitud del coseno (1 - similitud) para todos los pares de ítems\n","    dissimilarity_sum = 0\n","    num_pairs = 0\n","    for i in range(len(item_vectors)):\n","        for j in range(i + 1, len(item_vectors)):\n","            sim = cosine_similarity([item_vectors[i]], [item_vectors[j]])[0][0]\n","            dissimilarity_sum += (1 - sim)\n","            num_pairs += 1\n","\n","    return dissimilarity_sum / num_pairs if num_pairs > 0 else 0.0\n","\n"],"metadata":{"id":"fllurWHM68UF","executionInfo":{"status":"ok","timestamp":1764995658311,"user_tz":180,"elapsed":5,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}}},"id":"fllurWHM68UF","execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"ee022c0b","metadata":{"id":"ee022c0b","executionInfo":{"status":"ok","timestamp":1764995663129,"user_tz":180,"elapsed":4812,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}}},"outputs":[],"source":["df_train.drop_duplicates(inplace=True, subset=['user', 'item'])\n","df_val.drop_duplicates(inplace=True, subset=['user', 'item'])"]},{"cell_type":"code","source":["print(f\"Tamaño original del training set: {len(df_train)}\")\n","\n","# se obtiene un sample debido a que hay muchos datos y se demora mucho\n","df_train_sample = df_train.sample(n=1000000, random_state=42)\n","print(f\"Tamaño del nuevo training set (muestra): {len(df_train_sample)}\")\n","\n","# se obtiene un sample debido a que hay muchos datos y se demora mucho\n","df_val_sample = df_val.sample(n=50000, random_state=42)\n","print(f\"Tamaño del nuevo validation set (muestra): {len(df_val_sample)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uT2QRQhMFTZ7","executionInfo":{"status":"ok","timestamp":1764995664603,"user_tz":180,"elapsed":1464,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"dfbf21a5-20c6-499d-9fd9-96a51d93f71b"},"id":"uT2QRQhMFTZ7","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño original del training set: 10200445\n","Tamaño del nuevo training set (muestra): 1000000\n","Tamaño del nuevo validation set (muestra): 50000\n"]}]},{"cell_type":"code","source":["def evaluar_random_topn(df_train, df_val, n=10, sample_per_user=50):\n","    \"\"\"\n","    Genera recomendaciones aleatorias para cada usuario, tomando un sample\n","    limitado de items no vistos para evitar usar toda la matriz.\n","    \"\"\"\n","    # Diccionario {usuario: items que ha visto}\n","    user2seen = df_train.groupby('user')['item'].apply(set).to_dict()\n","\n","    # Lista de todos los items\n","    all_items = df_train['item'].unique().tolist()\n","\n","    top_n = {}\n","    for uid in df_val['user'].unique():\n","        seen = user2seen.get(uid, set())\n","        # Items posibles para recomendar\n","        candidates = list(set(all_items) - seen)\n","        # Tomar un sample limitado\n","        sample_candidates = random.sample(candidates, min(sample_per_user, len(candidates)))\n","        # Tomar n recomendaciones aleatorias\n","        recs = random.sample(sample_candidates, min(n, len(sample_candidates)))\n","        top_n[uid] = [(iid, random.randint(1,5)) for iid in recs]\n","\n","    return top_n\n","\n","\n","# Medir tiempo de ejecución\n","start_time = time.time()\n","top_n = evaluar_random_topn(df_train_sample, df_val_sample, n=10)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Tiempo de ejecución: {elapsed_time:.2f} segundos\")\n","\n","# Medir la memoria utilizada\n","memoria = memory_usage(\n","    (evaluar_random_topn, (df_val_sample, df_val_sample), {'n':10})\n",")\n","print(\"Memoria usada (MB):\", max(memoria) - min(memoria))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epNk4GNmBVct","executionInfo":{"status":"ok","timestamp":1764995741672,"user_tz":180,"elapsed":77053,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"63d09ef0-ef6a-45b9-db77-6c364a744ab6"},"id":"epNk4GNmBVct","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Tiempo de ejecución: 51.87 segundos\n","Memoria usada (MB): 36.90625\n"]}]},{"cell_type":"code","source":["def rmse_mae_from_topn(top_n, df_val_sample):\n","    real, predicho = [], []\n","    total = sum(len(recs) for recs in top_n.values())\n","    i = 0\n","    for uid, recs in top_n.items():\n","      for iid, pred in recs:\n","        real_vals = df_val_sample.loc[(df_val_sample['user'] == uid) & (df_val_sample['item'] == iid), 'rating']\n","        if not real_vals.empty:\n","            real.append(real_vals.values[0])\n","            predicho.append(pred)\n","        i += 1\n","        if i % 10000 == 0 or i == total:  # muestra cada 100 pasos o al final\n","            progreso = (i / total) * 100\n","            print(f\"Progreso: {i}/{total} ({progreso:.2f}%)\")\n","\n","    return math.sqrt(mean_squared_error(real, predicho)), mean_absolute_error(real, predicho)\n","\n","rmse, mae = rmse_mae_from_topn(top_n, df_val_sample)\n","print(\"RMSE para las top n recomendaciones\", rmse)\n","print(\"MAE para las top n recomendaciones\", mae)\n"],"metadata":{"id":"sQptPHUXIuPi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765000606657,"user_tz":180,"elapsed":4864994,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"027bc946-ed1c-4914-d13f-a3286a73ef68"},"id":"sQptPHUXIuPi","execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Progreso: 10000/375480 (2.66%)\n","Progreso: 20000/375480 (5.33%)\n","Progreso: 30000/375480 (7.99%)\n","Progreso: 40000/375480 (10.65%)\n","Progreso: 50000/375480 (13.32%)\n","Progreso: 60000/375480 (15.98%)\n","Progreso: 70000/375480 (18.64%)\n","Progreso: 80000/375480 (21.31%)\n","Progreso: 90000/375480 (23.97%)\n","Progreso: 100000/375480 (26.63%)\n","Progreso: 110000/375480 (29.30%)\n","Progreso: 120000/375480 (31.96%)\n","Progreso: 130000/375480 (34.62%)\n","Progreso: 140000/375480 (37.29%)\n","Progreso: 150000/375480 (39.95%)\n","Progreso: 160000/375480 (42.61%)\n","Progreso: 170000/375480 (45.28%)\n","Progreso: 180000/375480 (47.94%)\n","Progreso: 190000/375480 (50.60%)\n","Progreso: 200000/375480 (53.27%)\n","Progreso: 210000/375480 (55.93%)\n","Progreso: 220000/375480 (58.59%)\n","Progreso: 230000/375480 (61.25%)\n","Progreso: 240000/375480 (63.92%)\n","Progreso: 250000/375480 (66.58%)\n","Progreso: 260000/375480 (69.24%)\n","Progreso: 270000/375480 (71.91%)\n","Progreso: 280000/375480 (74.57%)\n","Progreso: 290000/375480 (77.23%)\n","Progreso: 300000/375480 (79.90%)\n","Progreso: 310000/375480 (82.56%)\n","Progreso: 320000/375480 (85.22%)\n","Progreso: 330000/375480 (87.89%)\n","Progreso: 340000/375480 (90.55%)\n","Progreso: 350000/375480 (93.21%)\n","Progreso: 360000/375480 (95.88%)\n","Progreso: 370000/375480 (98.54%)\n","Progreso: 375480/375480 (100.00%)\n","RMSE para las top n recomendaciones 4.613334709606181\n","MAE para las top n recomendaciones 4.257142857142857\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import ndcg_score\n","\n","# --- Crear DataFrame de Evaluación con Scores Aleatorios ---\n","print(\"Creando DataFrame de evaluación para el modelo 'Random'...\")\n","# Usamos la muestra del set de validación, como en tu notebook original\n","df_eval = df_val_sample.copy()\n","df_eval = df_eval.rename(columns={'user': 'userID', 'item': 'itemID'})\n","\n","# Creamos el 'label' para saber si el ítem es relevante\n","df_eval['label'] = (df_eval['rating'] >= 7).astype(int)\n","# Asignamos un score completamente aleatorio a cada interacción\n","np.random.seed(42)\n","df_eval['score'] = np.random.rand(len(df_eval))\n","print(\"DataFrame de evaluación aleatorio creado con éxito.\")\n","\n","# --- Funciones de métrica de ranking (puedes moverlas si ya las tienes en otra celda) ---\n","def precision_recall_at_k(group, k):\n","    group = group.sort_values('score', ascending=False)\n","    topk = group.head(k)\n","    hits = topk['label'].sum()\n","    total_relevant = group['label'].sum()\n","    precision = hits / k if k > 0 else 0\n","    recall = hits / total_relevant if total_relevant > 0 else 0\n","    return precision, recall\n","\n","def ndcg_at_k(group, k):\n","    if group['label'].sum() == 0: return 0.0\n","    ranked_group = group.sort_values('score', ascending=False).head(k)\n","    if len(ranked_group) < 2: return 0.0\n","    true_relevance = np.asarray([ranked_group['label'].values])\n","    predicted_scores = np.asarray([ranked_group['score'].values])\n","    return ndcg_score(true_relevance, predicted_scores)\n","\n","\n","# --- Evaluación Individual Completa ---\n","K_values = [10]\n","individual_results = []\n","print(\"\\nCalculando métricas individuales para Random...\")\n","\n","grouped_users = df_eval.groupby('userID')\n","\n","for k in K_values:\n","    metrics = grouped_users.apply(lambda x: precision_recall_at_k(x, k))\n","    avg_precision = np.mean([m[0] for m in metrics])\n","    avg_recall = np.mean([m[1] for m in metrics])\n","    avg_ndcg = grouped_users.apply(lambda x: ndcg_at_k(x, k)).mean()\n","    avg_novelty = grouped_users.apply(lambda x: novelty_at_k(x, k, item_popularity_prob)).mean()\n","    avg_diversity = grouped_users.apply(lambda x: diversity_at_k(x, k, df_mechanics)).mean()\n","\n","    individual_results.append({\n","        'K': k,\n","        'Precision@K': avg_precision,\n","        'Recall@K': avg_recall,\n","        'nDCG@K': avg_ndcg,\n","        'Novelty@K': avg_novelty,\n","        'Diversity@K': avg_diversity\n","    })\n","\n","individual_results_df = pd.DataFrame(individual_results)\n","print(\"\\n--- Resultados de Evaluación Individual (Random) ---\")\n","print(individual_results_df)"],"metadata":{"id":"OEFQiXSH7HSJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765000693850,"user_tz":180,"elapsed":87191,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"d5fe35b2-4990-4311-c9aa-e07fbbb4842e"},"id":"OEFQiXSH7HSJ","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Creando DataFrame de evaluación para el modelo 'Random'...\n","DataFrame de evaluación aleatorio creado con éxito.\n","\n","Calculando métricas individuales para Random...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3113504245.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  metrics = grouped_users.apply(lambda x: precision_recall_at_k(x, k))\n","/tmp/ipython-input-3113504245.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_ndcg = grouped_users.apply(lambda x: ndcg_at_k(x, k)).mean()\n","/tmp/ipython-input-3113504245.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_novelty = grouped_users.apply(lambda x: novelty_at_k(x, k, item_popularity_prob)).mean()\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Resultados de Evaluación Individual (Random) ---\n","    K  Precision@K  Recall@K    nDCG@K  Novelty@K  Diversity@K\n","0  10     0.089331  0.745785  0.166149  11.324084     0.187942\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3113504245.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_diversity = grouped_users.apply(lambda x: diversity_at_k(x, k, df_mechanics)).mean()\n"]}]},{"cell_type":"markdown","source":["Ahora haremos lo mismo pero para un grupo de 4 personas que han calificado ese item (haremos 4 personas por mas que haya juegos que es de máximo 2 o 10 etc por simplicidad). Lo que hacemos es calcular el promedio real de las calificaciones de esas 4 personas y compararlo contra una predicción grupal generada de manera completamente aleatoria en el rango de ratings posibles."],"metadata":{"id":"U7hu4Q0nG96z"},"id":"U7hu4Q0nG96z"},{"cell_type":"code","source":["def evaluar_random_topn_grupos(df_train, df_val, n=10, sample_per_user=50):\n","    \"\"\"\n","    Genera recomendaciones aleatorias para cada usuario, tomando un sample\n","    limitado de items no vistos para evitar usar toda la matriz.\n","    \"\"\"\n","    # Diccionario {usuario: items que ha visto}\n","    user2seen = df_train.groupby('user')['item'].apply(set).to_dict()\n","\n","    # Lista de todos los items\n","    all_items = df_train['item'].unique().tolist()\n","\n","    top_n = {}\n","    # armar grupos\n","    usuarios = df_val['user'].unique()\n","    grupos = [usuarios[i:i+4] for i in range(0, len(usuarios) - len(usuarios)%4, 4)]\n","\n","    seen_group = set()\n","    for grupo in grupos:\n","      for u in grupo:\n","          seen_group |= user2seen.get(u, set())\n","          # Items posibles para recomendar\n","          candidates = list(set(all_items) - seen_group)\n","          # Tomar un sample limitado\n","          sample_candidates = random.sample(candidates, min(sample_per_user, len(candidates)))\n","          # Tomar n recomendaciones aleatorias\n","          recs = random.sample(sample_candidates, min(n, len(sample_candidates)))\n","          top_n[tuple(grupo)] = [(iid, random.randint(1,5)) for iid in recs]\n","\n","    return top_n\n","\n","\n","# Medir tiempo de ejecución\n","start_time = time.time()\n","top_n_grupo = evaluar_random_topn_grupos(df_train_sample, df_val_sample, n=10)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(f\"Tiempo de ejecución: {elapsed_time:.2f} segundos\")\n","\n","# Medir la memoria utilizada\n","memoria = memory_usage(\n","    (evaluar_random_topn_grupos, (df_train_sample, df_val_sample), {'n':10})\n",")\n","print(\"Memoria usada (MB):\", max(memoria) - min(memoria))"],"metadata":{"id":"epkNsTyrKxkT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765000825661,"user_tz":180,"elapsed":131698,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"8cb850c3-033c-4d41-d200-58c0db6acda8"},"id":"epkNsTyrKxkT","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Tiempo de ejecución: 63.00 segundos\n","Memoria usada (MB): 108.1875\n"]}]},{"cell_type":"code","source":["def rmse_mae_from_topn_grupo(top_n_grupo, df_val_sample):\n","    real, predicho = [], []\n","    total = sum(len(recs) for recs in top_n.values())\n","    i = 0\n","    for grupo, recs in top_n_grupo.items():\n","      for iid, pred in recs:\n","        real_vals = df_val_sample.loc[(df_val_sample['user'].isin(grupo)) & (df_val_sample['item'] == iid), 'rating']\n","        if not real_vals.empty:\n","            real.append(real_vals.values[0])\n","            predicho.append(pred)\n","        i += 1\n","        if i % 10000 == 0 or i == total:  # muestra cada 100 pasos o al final\n","            progreso = (i / total) * 100\n","            print(f\"Progreso: {i}/{total} ({progreso:.2f}%)\")\n","\n","    return math.sqrt(mean_squared_error(real, predicho)), mean_absolute_error(real, predicho)\n","\n","rmse_grupo, mae_grupo = rmse_mae_from_topn_grupo(top_n_grupo, df_val_sample)\n","print(\"RMSE para las top n recomendaciones\", rmse_grupo)\n","print(\"MAE para las top n recomendaciones\", mae_grupo)\n"],"metadata":{"id":"qc24swLLJkoV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765001669093,"user_tz":180,"elapsed":843444,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"outputId":"4cf863ea-a2f1-418a-eb08-12cb73fa7abb"},"id":"qc24swLLJkoV","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Progreso: 10000/375480 (2.66%)\n","Progreso: 20000/375480 (5.33%)\n","Progreso: 30000/375480 (7.99%)\n","Progreso: 40000/375480 (10.65%)\n","Progreso: 50000/375480 (13.32%)\n","Progreso: 60000/375480 (15.98%)\n","Progreso: 70000/375480 (18.64%)\n","Progreso: 80000/375480 (21.31%)\n","Progreso: 90000/375480 (23.97%)\n","RMSE para las top n recomendaciones 2.2638462845343543\n","MAE para las top n recomendaciones 2.25\n"]}]},{"cell_type":"markdown","source":["Los códigos para random se adaptaron de un codigo inicial creado, la adaptación de este código se encuentra aquí: https://chatgpt.com/share/68e00c31-dbf8-8006-bacd-84f0296d467c\n"],"metadata":{"id":"1Wz3zHqTJ2pV"},"id":"1Wz3zHqTJ2pV"},{"cell_type":"code","source":["from sklearn.metrics import ndcg_score\n","\n","# Asegúrate de que df_eval está definido como en el paso anterior.\n","# df_eval ya contiene las columnas: userID, itemID, rating, label, y score aleatorio.\n","\n","print(\"\\nCreando grupos sintéticos...\")\n","# Usamos solo usuarios con al menos 10 interacciones para formar grupos más robustos\n","user_counts = df_eval['userID'].value_counts()\n","valid_users = user_counts[user_counts >= 10].index.tolist()\n","\n","np.random.seed(42)\n","num_groups = 1000\n","group_size = 4\n","# Asegúrate de que hay suficientes usuarios válidos para crear los grupos\n","if len(valid_users) < group_size * num_groups:\n","    print(f\"Advertencia: No hay suficientes usuarios únicos ({len(valid_users)}) para crear {num_groups} grupos sin reemplazo. Se crearán menos grupos.\")\n","    num_groups = len(valid_users) // group_size\n","\n","groups = [np.random.choice(valid_users, group_size, replace=False) for _ in range(num_groups)]\n","print(f\"Se crearon {len(groups)} grupos sintéticos de tamaño {group_size}.\")\n","\n","\n","print(\"\\nAgregando predicciones para cada grupo...\")\n","all_group_recs = []\n","for group_id, user_ids in enumerate(groups):\n","    group_predictions = df_eval[df_eval['userID'].isin(user_ids)]\n","    item_scores_per_group = group_predictions.groupby('itemID').agg(\n","        avg_score=('score', 'mean'),\n","        min_score=('score', 'min'),\n","        max_score=('score', 'max'),\n","        group_label=('label', lambda x: 1 if all(x == 1) else 0)\n","    ).reset_index()\n","    item_scores_per_group['group_id'] = group_id\n","    all_group_recs.append(item_scores_per_group)\n","\n","df_group_eval = pd.concat(all_group_recs, ignore_index=True)\n","print(\"Agregación completada.\")\n","\n","# --- Evaluación de Estrategias con Todas las Métricas ---\n","strategies = {\n","    'Average': 'avg_score',\n","    'Least Misery': 'min_score',\n","    'Most Pleasure': 'max_score'\n","}\n","\n","group_results = []\n","K_values = [10]\n","\n","for strategy_name, score_column in strategies.items():\n","    print(f\"\\nEvaluando estrategia (Random): {strategy_name}...\")\n","    df_strategy_eval = df_group_eval[['group_id', 'itemID', 'group_label']].copy()\n","    df_strategy_eval.rename(columns={'group_label': 'label'}, inplace=True)\n","    df_strategy_eval['score'] = df_group_eval[score_column]\n","\n","    grouped_strategy = df_strategy_eval.groupby('group_id')\n","\n","    for k in K_values:\n","        # Métricas existentes\n","        metrics = grouped_strategy.apply(lambda x: precision_recall_at_k(x, k))\n","        avg_precision = np.mean([m[0] for m in metrics])\n","        avg_recall = np.mean([m[1] for m in metrics])\n","        avg_ndcg = grouped_strategy.apply(lambda x: ndcg_at_k(x, k)).mean()\n","\n","        # Nuevas métricas\n","        avg_novelty = grouped_strategy.apply(lambda x: novelty_at_k(x, k, item_popularity_prob)).mean()\n","        avg_diversity = grouped_strategy.apply(lambda x: diversity_at_k(x, k, df_mechanics)).mean()\n","\n","        group_results.append({\n","            'Model': 'Random',\n","            'Strategy': strategy_name,\n","            'K': k,\n","            'Precision@K': avg_precision,\n","            'Recall@K': avg_recall,\n","            'nDCG@K': avg_ndcg,\n","            'Novelty@K': avg_novelty,\n","            'Diversity@K': avg_diversity\n","        })\n","\n","group_results_df = pd.DataFrame(group_results)\n","\n","print(\"\\n--- Resultados de Evaluación Grupal para Random ---\")\n","print(group_results_df)"],"metadata":{"id":"0nHeuu_oyCcC","executionInfo":{"status":"ok","timestamp":1765001669805,"user_tz":180,"elapsed":710,"user":{"displayName":"Mariela Zambrano","userId":"13626181033812741166"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0e26113-9b51-4750-9235-5c90a9ba9038"},"id":"0nHeuu_oyCcC","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Creando grupos sintéticos...\n","Advertencia: No hay suficientes usuarios únicos (20) para crear 1000 grupos sin reemplazo. Se crearán menos grupos.\n","Se crearon 5 grupos sintéticos de tamaño 4.\n","\n","Agregando predicciones para cada grupo...\n","Agregación completada.\n","\n","Evaluando estrategia (Random): Average...\n","\n","Evaluando estrategia (Random): Least Misery...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3059351082.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  metrics = grouped_strategy.apply(lambda x: precision_recall_at_k(x, k))\n","/tmp/ipython-input-3059351082.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_ndcg = grouped_strategy.apply(lambda x: ndcg_at_k(x, k)).mean()\n","/tmp/ipython-input-3059351082.py:65: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_novelty = grouped_strategy.apply(lambda x: novelty_at_k(x, k, item_popularity_prob)).mean()\n","/tmp/ipython-input-3059351082.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_diversity = grouped_strategy.apply(lambda x: diversity_at_k(x, k, df_mechanics)).mean()\n","/tmp/ipython-input-3059351082.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  metrics = grouped_strategy.apply(lambda x: precision_recall_at_k(x, k))\n","/tmp/ipython-input-3059351082.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_ndcg = grouped_strategy.apply(lambda x: ndcg_at_k(x, k)).mean()\n","/tmp/ipython-input-3059351082.py:65: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_novelty = grouped_strategy.apply(lambda x: novelty_at_k(x, k, item_popularity_prob)).mean()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Evaluando estrategia (Random): Most Pleasure...\n","\n","--- Resultados de Evaluación Grupal para Random ---\n","    Model       Strategy   K  Precision@K  Recall@K    nDCG@K  Novelty@K  \\\n","0  Random        Average  10         0.42  0.188332  0.628447  13.683819   \n","1  Random   Least Misery  10         0.42  0.188332  0.628447  13.683819   \n","2  Random  Most Pleasure  10         0.42  0.188332  0.628447  13.683819   \n","\n","   Diversity@K  \n","0     0.928452  \n","1     0.928452  \n","2     0.928452  \n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3059351082.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_diversity = grouped_strategy.apply(lambda x: diversity_at_k(x, k, df_mechanics)).mean()\n","/tmp/ipython-input-3059351082.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  metrics = grouped_strategy.apply(lambda x: precision_recall_at_k(x, k))\n","/tmp/ipython-input-3059351082.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_ndcg = grouped_strategy.apply(lambda x: ndcg_at_k(x, k)).mean()\n","/tmp/ipython-input-3059351082.py:65: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_novelty = grouped_strategy.apply(lambda x: novelty_at_k(x, k, item_popularity_prob)).mean()\n","/tmp/ipython-input-3059351082.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  avg_diversity = grouped_strategy.apply(lambda x: diversity_at_k(x, k, df_mechanics)).mean()\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.10"}},"nbformat":4,"nbformat_minor":5}